[package]
name = "ml-crate-dsrs"
version = "0.1.0"
edition = "2021"
authors = ["Omakase Gaming"]
description = "DSPy-RS model crate with llama.cpp adapter for GPU-accelerated LLM inference (Qwen2.5-0.5B)"
license = "MIT"

[dependencies]
# dspy-rs integration (core responsibility)
dspy-rs = "0.7.3"
async-trait = "0.1"

# Tokio async runtime
tokio = { version = "1", features = ["full"] }
futures = "0.3"  # For streaming support (Phase 2B)

# llama.cpp Rust bindings for multi-backend GPU support (AMD, NVIDIA, Intel, Apple)
# Supports: Vulkan (default), CUDA, Metal, CPU fallback
# See specs/03-multi-backend-strategy.md for details
llama-cpp-2 = "0.1"

# Utilities
thiserror = "1.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
anyhow = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = "0.3"  # For examples and tests

# Hot reload (Phase 3A-Optional)
notify = "6.1"                  # Cross-platform file watcher
notify-debouncer-mini = "0.4"   # Debounce rapid changes
sha2 = "0.10"                   # Hash validation for change detection

# Rhai scripting (Phase 3C)
rhai = "1.0"

# Sampling (Phase 1: for token sampling in generate())
rand = "0.8"

# Tool integration (required by dspy-rs Adapter trait)
# Note: rig-core is renamed to rig_core in imports
# We specify the package name explicitly to match dspy-rs's dependency
rig_core = { package = "rig-core", version = "0.22" }

# Optional: HuggingFace Hub for model downloads (Phase 1+)
hf-hub = { version = "0.3", optional = true }
regex = "1.12.2"

[dev-dependencies]
tokio-test = "0.4"
tempfile = "3.19"

[lib]
name = "ml_crate_dsrs"
path = "src/lib.rs"

# Examples - Phase 1
# [[example]]
# name = "quick_test"
# path = "examples/quick_test.rs"

# Future examples
# [[example]]
# name = "simple_qa"
# path = "examples/simple_qa.rs"

# [[example]]
# name = "chain_of_thought"
# path = "examples/chain_of_thought.rs"

# Feature flags for llama.cpp backend selection
# Default: Vulkan for broadest GPU support (AMD, NVIDIA, Intel)
[features]
default = ["vulkan"]

# GPU backends for llama-cpp-2
cuda = ["llama-cpp-2/cuda"]      # NVIDIA GPUs (+10-20% vs Vulkan)
vulkan = ["llama-cpp-2/vulkan"]  # AMD, NVIDIA, Intel GPUs (default)
metal = ["llama-cpp-2/metal"]    # Apple Silicon
cpu = []                          # CPU fallback (always available)
